**基于时间序列对金融数据的异常检测感知的研究**

# 摘要：

随着金融科技的快速发展和数字化转型的深入推进，金融机构的IT基础设施产生了海量的运营数据（非交易型数据），包括系统日志、API性能指标、数据质量度量、批处理作业状态以及OpenTelemetry标准下的分布式追踪数据等。这些运营数据呈现出典型的大规模时序特征：数据量可达百万至亿级样本、具有明显的时间依赖性和周期性模式（日周期、周周期、季节性）、存在概念漂移现象（数据分布随时间变化）、并对实时性要求极高。运营数据中的异常往往预示着系统故障、性能劣化、数据质量问题或潜在的安全风险，若不能及时发现和处理，将严重影响业务连续性和用户体验，甚至导致重大经济损失和监管合规问题。
传统的基于规则和阈值的异常检测方法在面对多维、非线性、动态变化的运营数据时表现出明显的局限性：规则难以穷尽所有异常模式、阈值设置依赖人工经验且无法自适应调整、难以平衡误报率和漏报率。采用无监督机器学习算法进行自动化异常检测成为金融机构亟待解决的关键技术挑战。

关键词：异常检测；金融运营数据；Open Telemetry
中图分类号：   文献标识码：  

# Research on Anomaly Detection and Perception of Financial Data Based on Time Series

Diao Yong
(Library and Archives, Dalian Maritime University, Dalian, Liaoning 116026)

# Abstract:
With the rapid advancement of fintech and the deepening of digital transformation, financial institutions' IT infrastructure generates massive amounts of operational data (non-transactional data). This includes system logs, API performance metrics, data quality measurements, batch job statuses, and distributed tracing data under the Open Telemetry standard. This operational data exhibits typical large-scale time-series characteristics: data volumes reaching millions to billions of samples, pronounced time dependency and periodic patterns (daily cycles, weekly cycles, seasonality), conceptual drift (data distribution shifts over time), and extremely high real-time requirements. Anomalies in operational data often signal system failures, performance degradation, data quality issues, or potential security risks. Failure to detect and address these promptly can severely impact business continuity and user experience, potentially leading to significant financial losses and regulatory compliance issues.
Traditional rule-based and threshold-based anomaly detection methods exhibit significant limitations when applied to multidimensional, nonlinear, and dynamically changing operational data: rules cannot exhaustively cover all anomaly patterns; threshold settings rely on manual expertise and lack adaptive adjustment capabilities; and false positive and false negative rates are difficult to balance. Implementing unsupervised machine learning algorithms for automated anomaly detection has become a critical technological challenge that financial institutions urgently need to address.

Keywords: Anomaly Detection; Financial Operations Data; Open Telemetry

# 1. 引言(Introduction)

金融机构IT系统生成的运营数据（系统日志、API性能、数据质量指标等）呈现高维时序特征和极度不平衡分布（异常样本少于5%）。传统基于规则和阈值的方法在应对动态环境时，面临规则维护成本高、静态阈值误报率高、高维特征交互难以覆盖等等问题。本文提出基于多层混合策略的无监督异常检测框架，通过快速筛选、深度分析和自适应融合三阶段分析，在保持高召回率的同时限制降低误报率和计算成本开销。

# 2. 传统异常检测方法的局限性与挑战

在金融运营数据场景中，传统的基于规则和固定阈值的异常检测方法面临以下关键挑战：

- 规则维护成本高昂： 传统方法依赖人工定义的业务规则（如“CPU使用率>80%持续5分钟触发告警”），但金融系统的复杂度不断提升，规则数量呈指数级增长。据统计，大型金融机构的监控规则数量可达数万条，维护成本极高且容易产生规则冲突。
- 静态阈值难以适应动态环境： 金融业务存在明显的周期性和季节性特征（如月末、季末、年末的批处理高峰），静态阈值无法自适应这些变化，导致在正常业务高峰期产生大量误报（False Positives），造成“告警疲劳”（Alert Fatigue），降低运维团队的响应效率。
- 高维数据的“维度灾难”： 现代金融系统的可观测性数据维度极高（可达数百至上千维特征），传统规则难以覆盖特征间的复杂交互关系。例如，单独的“API延迟高”或“数据库查询慢”可能不足以判定异常，但两者同时出现且伴随特定的流量模式时，则很可能预示着严重的系统问题。
- 人工标注数据稀缺： 在运营监控场景中，获取高质量的标注异常数据（Labeled Data）成本极高，且异常类型多样、分布不均衡（异常样本通常占比不到1-5%），使得监督学习方法难以有效应用。

因此，全自动、高精度、可解释的无监督异常检测技术成为金融机构迫切需要攻克的技术难题。


# 3. 运营数据的核心特征

金融运营数据 (Financial Operational Data) 区别于交易数据 (Transactional Data)，主要指金融机构IT基础设施和业务系统在运行过程中产生的监控、日志、度量数据。这类数据呈现出显著的时间序列特性，数据点按时间顺序生成且相邻时间点之间存在强相关性。系统负载在工作时间和非工作时间的差异清晰可见，同时存在多层次的周期性模式，包括日周期的24小时循环、工作日与周末的周周期差异、月初月中月末的业务高峰月周期，以及财报季和审计季带来的年度周期性。长期观察这些数据往往能发现明显的趋势性，例如随着业务规模增长，系统负载呈现持续上升的态势。

运营数据的单个监控对象通常包含数十至数百个监控指标，而整个系统级别的特征维度可达数千。这些数据类型涵盖连续型数值如CPU使用率和内存占用、离散型计数如错误次数和请求数、以及类别型标签如日志级别和服务状态。不同指标之间的量纲差异极大，响应时间以毫秒为单位，而吞吐量可能达到万级QPS，这种差异性使得数据标准化成为必不可少的预处理步骤。

大型金融机构每天产生的运营日志和监控数据可达TB至PB级别，样本数量从百万级到亿级不等。这些数据以流式方式持续产生，对采集、传输和处理的实时性要求达到秒级甚至亚秒级。传统的单机处理能力已远不能满足需求，必须依赖分布式存储和计算框架来应对这种规模的数据处理挑战。

概念漂移 (Concept Drift) 是运营数据面临的另一个关键问题。数据分布并非静态不变，而是随时间动态演化。系统升级、业务扩展、架构调整都会导致正常数据的统计特性发生改变，新的异常类型如新型攻击手段和未知故障模式也在不断出现，使得基于历史数据训练的模型可能逐渐失效。这要求检测模型必须具备在线学习或增量更新能力，能够自适应地跟随数据分布的变化。

运营数据中的噪声与不确定性进一步增加了异常检测的难度。传感器误差、网络抖动、采样偏差等因素导致数据存在固有噪声。更棘手的是，异常的定义本身具有主观性，不同业务场景对异常的容忍度各不相同，且缺乏权威的标注基准。此外，正常样本远多于异常样本的类别不平衡现象普遍存在，异常占比通常低于5%，这使得传统分类方法容易偏向多数类而忽视异常模式的识别。

# 4 标准化与形式化定义以及策略分析

## 4.1 基于OpenTelemetry的数据标准化定义

为实现跨系统、跨平台的统一监控和异常检测，本文建议金融企业采用OpenTelemetry作为数据采集和规范化标准。OpenTelemetry是CNCF (Cloud Native Computing Foundation) 孵化的开源可观测性框架，已成为业界事实标准。

本文对以下15维金融运营数据特征集进行试验，其中传统运营指标8维, OpenTelemetry服务端指标7维：

- API响应时间（毫秒），反映服务端性能
- 批处理作业延迟（秒），衡量数据处理效率
- 错误率（百分比），系统健康度核心指标
- 吞吐量（记录数/分钟），反映业务处理能力
- CPU使用率（百分比），资源利用率
- 内存使用率（百分比），反映了资源利用率
- 数据质量分数（0-100），综合评估数据完整性、准确性、一致性
- 并发连接数，是系统负载指标
- HTTP请求P95分位延迟（毫秒），捕捉长尾效应
- 数据库查询时间（毫秒），反映后端性能瓶颈
- 缓存命中率（百分比），体现缓存效率
- 消息队列延迟（毫秒），体现异步处理性能
- 分布式追踪Span时长（毫秒），反映微服务调用耗时
- 服务查询每秒 (QPS)，是流量指标
- HTTP 5xx错误率（百分比），反映了服务端故障率

## 4.2 异常类型的形式化定义

根据Chandola等人的经典分类框架（IEEE TKDE 2009）和金融运营场景的实际需求，本文将异常分为三大类：

**点异常 (Point Outliers / Global Outliers)**

点异常指在多维特征空间中，单个数据点是显著偏离正常数据的个体分布。这是最常见的异常类型，判断标准是数据点的异常得分是否超过预设阈值。点异常的检测不考虑时间顺序和上下文信息，仅着重于单点的统计偏离程度。在金融运营场景中，点异常经常出现。例如，某个交易日上下午2点，核心交易系统的一台服务器CPU使用率从平时的50%左右突然跃升到95%。这种明显偏离正常运行区间的单点数据是典型的点异常，这类异常预示着系统正在经历突发故障或遭受攻击。

**情境异常 (Contextual Outliers / Conditional Outliers)**

情境异常的特殊之处在于数据点在特定上下文中间表现异常，但在全局或其它上下文中间可能完全正常。这里的上下文通常由时间、空间或业务场景等条件变量定义。情境异常的识别需要同时满足两个条件：在特定上下文中的异常得分超过上下文阈值，但是全局异常得分并未超过全局阈值。金融机构的运营数据中充斥着这类与时间强相关的异常。某大型银行的批处理系统在每月最后一个工作日晚上都会经历交易量和计算负载的高峰，这是月末对账和结算的正常业务模式，但如果在月中某个平常工作日的深夜出现同等规模的批处理负载，则明显是不符合业务规律。

**集合异常 (Collective Outliers / Pattern Outliers)**

集合异常指数据点的一组连续或相关的数据点整体表现出的异常模式，即使这些数据点单独来看可能都在正常范围内。集合异常的判定依据是整个子序列的综合得分，而非单个点的得分。这是三类异常中检测难度最高的，因为需要同时考虑数据点之间的时间序列、空间关联或逻辑关联关系。在实际运营中，集合异常往往比单点异常更加隐蔽和危险。数据库团队可能观察到SQL查询的出现频率在过去一周内持续攀升，尽管每次查询的执行时间都在5秒以内的可接受范围，但这种趋势性的累积预示着系统资源或数据库瓶颈问题。网络安全团队面对的DDoS攻击也是典型的集合异常，每个HTTP请求单独看都是合法的，但当数万个请求在短时间内集中到达时，其时序模式就暴露了攻击特征。

## 4.3 异常得分与阈值决策机制

统一异常得分 (Outlier Score) 定义： 

为支持多算法集成和实时决策，本文定义归一化的异常得分 $s(x) \in [0, 1]$，其中：
- $s(x) \to 0$：高度正常
- $s(x) \to 1$：高度异常
- $s(x) \approx 0.5$：边界样本，需进一步判断

动态阈值策略： 

静态阈值（如 $\tau = 0.5$）难以适应数据分布变化，本文采用动态阈值策略：

1. 基于Contamination Rate的阈值： 根据历史异常率 $\alpha$ (通常设为0.05-0.10)，选取 $(1 - \alpha) \times 100$ 百分位数作为阈值：

    $$\tau = \text{Percentile}(S(D), (1 - \alpha) \times 100)$$

2. 基于业务影响的分级阈值：

- 低优先级 ($\tau_1 < s(x) < \tau_2$)：记录日志，不触发告警
- 中优先级 ($\tau_2 \le s(x) < \tau_3$)：发送邮件或工单告警
- 高优先级 ($s(x) \ge \tau_3$)：立即通过短信或电话告警，启动应急响应

3. 自适应滑动窗口阈值： 

基于最近 $N$ 个时间窗口（如过去7天）的数据分布动态调整阈值，适应概念漂移。

# 5. 算法描述与实际应用

## 5.1 三阶段分层异常检测策略算法概述

我们基于金融领域业务特点，设计了设计一个结合统计、卷积和循环网络优势的**三阶段多层检测策略**。该策略旨在结合快速筛选（统计/传统机器学习方法）和深度上下文建模（混合深度学习模型）的优势，以提高检测的准确性和对复杂异常模式（如局部异常和片段异常）的敏感性。


我们设计的策略包含三个主要阶段：

| 阶段 | 目的 | 推荐算法类别 | 算法优势利用 |
| :--- | :--- | :--- | :--- |
| **阶段 1：快速初筛** | 识别全局显著离群点，提供快速、可解释的初筛分数。 | 统计模型 / 传统机器学习 | **速度快、可解释性强**（如 HBOS, Z-score, Isolation Forest）。 |
| **阶段 2：深度上下文分析** | 捕捉局部特征和长期时间依赖性，进行精细的上下文异常建模。 | 混合深度学习（CNN-RNN）| **结合 CNN 提取局部模式的能力**和 **RNN 捕捉时间依赖关系的能力**。 |
| **阶段 3：策略整合与决策** | 融合两层分数，通过动态阈值进行最终的异常判定。 | 集成/阈值优化 | 融合不同模型对异常的互补视图。|

| 模型类型 | 优点 (Pros) | 缺点 (Cons) | 在多层策略中的作用 | 来源 |
| :--- | :--- | :--- | :--- | :--- |
| **统计模型 (Z-score)** | **简单易懂，计算效率高**。 | 假设数据服从正态分布，在非正态分布下准确性受限。 | 快速识别**全局点异常**。 | |
| **卷积神经网络 (CNN)** | 擅长提取**局部特征**或“空间关系”，参数共享减少模型复杂度。 | 对于**长期依赖性**较强的时间序列表现不如 RNN。 | 提取输入时间窗口内的**局部模式**和**形状特征**。 | |
| **循环神经网络 (RNN/LSTM)** | 适合从数据中学习**时间依赖关系**，能够处理长序列。 | 训练较慢，容易出现梯度消失或爆炸（LSTM/GRU 有所缓解）。 | 捕捉序列的**动态特性**和**长期上下文信息**。 | |
| **混合模型 (CNN-RNN)** | **互补优势**，同时学习空间和时间特征，提高性能。 | 架构复杂，训练资源需求增加。 | 实现**上下文异常检测**和**片段异常**的识别。 | |

## 数学公式与实现框架

我们采用基于重构或预测误差的异常检测范式作为核心机制，并辅以统计检测。这个设计类似于在时间序列异常检测中广泛应用的 **SALAD 模型** (Stochastic Adversarial Learned Anomaly Detection)，该模型也结合了自编码器和对抗网络来更好地建模复杂时序分布，并使用重构误差和判别误差的组合来计算异常分数。SALAD 的最终异常分数定义为重构误差和判别误差的加权和。我们的策略阶段三 $S_{final}$ 就借鉴了这种融合思想，将统计离群点检测 $A_1$ 替换为 SALAD 中的判别误差 $\mathcal{D}(\hat{x})$，以实现不同层级的检测集成。

### 阶段 1：全局点异常分数 $A_1$ (基于 Z-score)

该阶段用于检测点异常，特别是全局点异常，即显著偏离整体数据分布的单个数据点。

**计算公式：**
$$
A_1(x_t) = \frac{|x_t - \mu_X|}{\sigma_X}
$$

其中：
*   $x_t$ 是 $t$ 时刻的观测值。
*   $\mu_X$ 是训练时间序列 $X$ 的均值。
*   $\sigma_X$ 是训练时间序列 $X$ 的标准差。

**初筛决策：**
如果 $A_1(x_t)$ 超过预设的统计阈值 $\lambda_1$（例如，$\lambda_1=3$，对应 $3\sigma$ 原则），则 $x_t$ 被标记为潜在异常。

$$
D_1(x_t) = \begin{cases} 1 & \text{if } A_1(x_t) > \lambda_1 \\ 0 & \text{otherwise} \end{cases}
$$

### 阶段 2：深度上下文异常分数 $A_2$ (基于 CNN-LSTM 重构误差)

该阶段利用混合模型（CNN 作为编码器，LSTM 作为解码器或建模时间依赖）来学习正常时间序列窗口 $w_t$ 的分布，并计算其重构误差 $A_2$。这有助于识别局部点异常或片段异常，因为模型难以精确重构偏离正常模式的异常片段。

**模型架构**：
我们采用 CNN-LSTM 混合自编码器架构。

1.  **输入窗口**：时间窗口 $w_t = (x_{t-\tau+1}, \dots, x_t)$，长度为 $\tau$。
2.  **编码器 (CNN)**：提取局部特征 $f_t^{CNN}$。
3.  **时间建模 (LSTM)**：处理 CNN 输出序列，捕捉时序依赖 $h_t^{LSTM}$。
4.  **解码器**：重构输出 $\hat{w}_t$。

**重构误差分数 (Mean Squared Error, MSE)**：

$$
A_2(w_t) = \text{MSE}(w_t, \hat{w}_t) = \frac{1}{\tau} \sum_{i=0}^{\tau-1} (x_{t-i} - \hat{x}_{t-i})^2
$$

其中，$\hat{w}_t = f_\theta(w_t)$ 是由参数 $\theta$ 的模型 $f_\theta$ 重构的输出窗口。

### 阶段 3：多层整合与最终决策 $S_{final}$

最终的检测分数 $S_{final}$ 是两个阶段分数的加权融合。在融合之前，需要对分数进行归一化（例如，使用 Min-Max 归一化或 Z-score 归一化），确保它们在相似的尺度上。

**归一化分数 (假设使用 Min-Max 归一化)：**
$$
\text{Normalize}(A) = \frac{A - \min(A)}{\max(A) - \min(A)}
$$

**融合异常分数：**
$$
S_{final}(x_t) = \alpha \cdot \text{Normalize}(A_1(x_t)) + (1 - \alpha) \cdot \text{Normalize}(A_2(w_t))
$$
其中 $\alpha \in$ 是权重超参数，用于控制统计初筛和深度上下文分析的相对重要性。

**最终检测决策：**
通过一个最终的自适应阈值 $\phi_{final}$ 进行判定。

$$
D_{final}(x_t) = \begin{cases} 1 & \text{if } S_{final}(x_t) \geq \phi_{final} \\ 0 & \text{otherwise} \end{cases}
$$
$\phi_{final}$ 可以通过在验证集上搜索最佳 $F_1$ 分数（或根据业务容忍度）来确定。


## 5.3 异常检测监控平台搭建

为支撑异常检测算法在生产环境的落地应用，需要构建一个端到端的监控平台。该平台采用OpenTelemetry作为统一的数据采集规范，将分布式系统中的追踪、日志和指标数据标准化提取。OpenTelemetry提供了与语言和厂商无关的可观测性框架，能够自动捕获应用程度的运行行为，并将其转换为结构化的运营数据，通过在各个服务节点部署OpenTelemetry Collector，系统可以实时收集包括请求延迟、错误率、资源利用率等在内的多维运营指标，这些指标经过标准化处理后形成了本文所使用的15维特征向量。

数据采集完成后，所有运营信息汇入Kafka消息队列中同时实现解耦和缓冲。异常检测模块从Kafka消费实时数据流。

所有经过检测的原始数据和异常评分最终持久化到数据湖（Data Lake）中，采用Parquet列式存储。数据湖不仅为运维团队提供了历史异常的回溯查询能力，还支持数据科学团队进行更复杂的离线分析。数据湖中的结构化数据可以直接对接商业智能工具，生成异常趋势报表、算法性能监控面板等可视化输出，为管理层的决策提供数据支撑。

# 6. 结论(Conclusion)

本文针对金融运营数据的异常检测需求，系统性研究了多种无监督异常检测算法在多维时间序列上的适用性和性能表现。这个多层策略利用了不同算法的固有特点：简单统计方法（如 Z-score）提供高效率和基本可解释性，而深度学习混合模型则专注于复杂和上下文相关的异常模式。通过融合它们的输出，可以创建一个更加鲁棒和全面的检测系统。

在工程实践层面，本文设计了基于opentelemetry和Kafka事件驱动的分布式监控平台架构，通过消息驱动机制实现了数据采集，流式异常检测和数据湖存储的完整解决方案。研究成果不仅为金融运营数据的实时异常监控提供了理论基础和技术支撑，也为其他业务领域的多维时间序列异常检测提供了借鉴。。


# 参考文献(References)

Cost of Data Center Outages: 
https://www.vertiv.com/globalassets/documents/reports/2016-cost-of-data-center-outages-11-11_51190_1.pdf

Chandola V, Banerjee A, Kumar V. Anomaly detection: a survey. ACM Comput Surv, 2009, 41: 1–58

IEEE TKDE：Transactions on Knowledge and Data Engineering
https://dl.acm.org/journal/ieeecs_tkde

PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection https://www.arxiv.org/abs/2412.12154

What is Opentelemetry: https://opentelemetry.io/docs/what-is-opentelemetry/

