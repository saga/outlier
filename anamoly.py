import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, precision_recall_curve
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf

from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.pca import PCA
from pyod.models.knn import KNN
from pyod.models.copod import COPOD
from pyod.models.auto_encoder import AutoEncoder
from pyod.models.ocsvm import OCSVM
from pyod.models.ecod import ECOD

# å¯¼å…¥æ·±åº¦å­¦ä¹ æ¨¡å‹
try:
    from pyod.models.auto_encoder import AutoEncoder
    AUTOENCODER_AVAILABLE = True
except ImportError:
    AUTOENCODER_AVAILABLE = False
# try:
#     from pyod.models.deep_svdd import DeepSVDD
# except ImportError:
#     print("DeepSVDDä¾èµ–ç¼ºå¤±, è·³è¿‡DeepSVDDæ¨¡å‹")
#     AUTOENCODER_AVAILABLE = False

import time
from datetime import datetime, timedelta


def generate_financial_operational_data(n_samples=10000, n_features=15, contamination=0.05):

    # ç”Ÿæˆé‡‘èè¿è¥æ•°æ® + OpenTelemetry æœåŠ¡å™¨é“¾è·¯ç›‘æ§æ•°æ®

    # æŒ‡æ ‡è¯´æ˜ï¼ˆ15ç»´ç‰¹å¾ï¼‰ï¼š
    # ä¼ ç»Ÿè¿è¥æŒ‡æ ‡ï¼ˆTraditional Operational Metricsï¼‰:
    # - API å“åº”æ—¶é—´(ms) -> API Response Time
    # - æ‰¹å¤„ç†å»¶æ—¶(ms) -> Batch Processing Delay
    # - é”™è¯¯ç‡(%) -> Error Rate
    # - ååé‡(records/min) -> Throughput
    # - CPU ä½¿ç”¨ç‡(%) -> CPU Usage
    # - å†…å­˜ä½¿ç”¨ç‡(%) -> Memory Usage
    # - æ•°æ®è´¨é‡è¯„åˆ†(0-100) -> Data Quality Score
    # - å¹¶å‘è¿æ¥æ•° -> Concurrent Connections

    # OpenTelemetry æœåŠ¡æŒ‡æ ‡ï¼ˆServer-side Metricsï¼‰:
    # - HTTP å¹³å‡å“åº”æ—¶é—´(ms) -> HTTP Request Latency P95
    # - CPU å³°å€¼ -> CPU Peak Usage
    # - æœåŠ¡è°ƒç”¨æŒç»­æ—¶é—´ -> Service Trace Duration
    # - é˜Ÿåˆ—æ—¶å»¶(ms) -> Queue latency
    # - Span Duration(ms) -> Distributed Trace Span Duration
    # - æ¯ç§’æŸ¥è¯¢æ•°(QPS) -> Service Queries Per Second
    # - HTTP é”™è¯¯ç‡(%) -> HTTP 5xx Error Rate

    np.random.seed(42)

    # æ­£å¸¸æ•°æ®
    n_normal = int(n_samples * (1 - contamination))
    n_anomaly = n_samples - n_normal

    # æ­£å¸¸é«˜æ–¯æ•°æ® - å¤šç»´æ­£æ€åˆ†å¸ƒ
    normal_data = np.random.randn(n_normal, n_features)

    # ä¼ ç»Ÿè¿è¥æŒ‡æ ‡å’ŒæœåŠ¡å™¨æŒ‡æ ‡é€é¡¹è°ƒæ•´åˆ†å¸ƒ
    # ç‰¹å¾ç»´åº¦æ˜ å°„ï¼ˆ0-7ï¼‰ï¼š
    # Feature 0: API å“åº”æ—¶é—´(50-200ms)
    normal_data[:, 0] = np.abs(normal_data[:, 0] * 30 + 125)

    # Feature 1: æ‰¹å¤„ç†å»¶æ—¶ (1-10s)
    normal_data[:, 1] = np.abs(normal_data[:, 1] * 2 + 5)

    # Feature 2: é”™è¯¯ç‡ (0-2%)
    normal_data[:, 2] = np.abs(normal_data[:, 2] * 0.5 + 1)

    # Feature 3: ååé‡ (800-1200 records/min)
    normal_data[:, 3] = np.abs(normal_data[:, 3] * 100 + 1000)

    # Feature 4: CPU ä½¿ç”¨ç‡ (30-70%)
    normal_data[:, 4] = np.abs(normal_data[:, 4] * 10 + 50)

    # Feature 5: å†…å­˜ä½¿ç”¨ç‡ (40-80%)
    normal_data[:, 5] = np.abs(normal_data[:, 5] * 10 + 60)

    # Feature 6: æ•°æ®è´¨é‡åˆ† (70-100)
    normal_data[:, 6] = np.abs(normal_data[:, 6] * 3 + 92)

    # Feature 7: å¹¶å‘è¿æ¥æ•° (50-200)
    normal_data[:, 7] = np.abs(normal_data[:, 7] * 25 + 100)

    # OpenTelemetry æœåŠ¡å™¨æŒ‡æ ‡ï¼ˆ8-14ï¼‰
    # Feature 8: HTTP å“åº” P95 (80-200ms)
    normal_data[:, 8] = np.abs(normal_data[:, 8] * 35 + 125)

    # Feature 9: CPU å³°å€¼(85-98%)
    normal_data[:, 9] = np.abs(normal_data[:, 9] * 10 + 27.5)

    # Feature 10: æœåŠ¡é˜Ÿåˆ—æ·±åº¦(5-20)
    normal_data[:, 10] = np.abs(normal_data[:, 10] * 3 + 9.6)

    # Feature 11: åˆ†å¸ƒå¼è¿½è¸ªè€—æ—¶(100-180ms)
    normal_data[:, 11] = np.abs(normal_data[:, 11] * 20 + 65)

    # Feature 12: Trace Span Duration (20-150ms)
    normal_data[:, 12] = np.abs(normal_data[:, 12] * 30 + 85)

    # Feature 13: QPS æ¯ç§’è°ƒç”¨é‡(800-1500 queries/sec)
    normal_data[:, 13] = np.abs(normal_data[:, 13] * 350 + 1260)

    # Feature 14: HTTP 5xx é”™è¯¯ç‡ (0-3%)
    normal_data[:, 14] = np.abs(normal_data[:, 14] * 0.1 + 0.25)


    # ç”Ÿæˆå¼‚å¸¸æ•°æ®ï¼ˆå¤šæºï¼‰
    # æç«¯å¼‚å¸¸ï¼šå®Œå…¨éšæœºå™ªå£°å¼‚å¸¸ï¼ˆ30%ï¼‰
    n_extreme = int(n_anomaly * 0.3)
    extreme_anomaly = np.random.randn(n_extreme, n_features) * 3 + 5
    
    # ç±»å‹2 å±€éƒ¨å¼‚å¸¸ï¼šéƒ¨åˆ†æŒ‡æ ‡åç§»å¼‚å¸¸ï¼ˆ30%ï¼‰
    n_local = int(n_anomaly * 0.3)
    local_anomaly = np.random.randn(n_local, n_features)
    # éšæœºé€‰æ‹©2ï½3ä¸ªç‰¹å¾ä¸ºå¼‚å¸¸
    for i in range(n_local):
        anomaly_features = np.random.choice(n_features, size=np.random.randint(2, 4), replace=False)
        local_anomaly[i, anomaly_features] *= 4

    # ç±»åˆ«3ï¼šæœåŠ¡ç«¯æ€§èƒ½åŠ£åŒ–å¼‚å¸¸ï¼ˆ20%ï¼‰ - æ¨¡æ‹ŸæœåŠ¡é™çº§
    n_service = int(n_anomaly * 0.2)
    service_anomaly = np.random.randn(n_service, n_features)

    # å‰8ä¸ªç‰¹å¾æ­£å¸¸
    for j in range(8):
        if j == 0:
            service_anomaly[:, j] = service_anomaly[:, j] * 30 + 125
        elif j == 1:
            service_anomaly[:, j] = np.abs(service_anomaly[:, j] * 2 + 5)
        elif j == 2:
            service_anomaly[:, j] = np.abs(service_anomaly[:, j] * 0.5 + 1)
        elif j == 3:
            service_anomaly[:, j] = service_anomaly[:, j] * 100 + 1000
        elif j == 4:
            service_anomaly[:, j] = service_anomaly[:, j] * 10 + 50
        elif j == 5:
            service_anomaly[:, j] = service_anomaly[:, j] * 10 + 60
        elif j == 6:
            service_anomaly[:, j] = service_anomaly[:, j] * 3 + 92
        elif j == 7:
            service_anomaly[:, j] = service_anomaly[:, j] * 25 + 100

    # OpenTelemetry æœåŠ¡ä¾§æŒ‡æ ‡å¼‚å¸¸ï¼ˆ8-14ï¼‰
    service_anomaly[:, 8]  = np.abs(service_anomaly[:, 8]  * 100 + 500)   # HTTP P95å»¶è¿Ÿé«˜
    service_anomaly[:, 9]  = np.abs(service_anomaly[:, 9]  * 15  + 90)    # æ•°æ®å³°å€¼é«˜
    service_anomaly[:, 10] = np.abs(service_anomaly[:, 10] * 25  + 50)    # é˜Ÿåˆ—æ—¶å»¶å‡é«˜
    service_anomaly[:, 11] = np.abs(service_anomaly[:, 11] * 30  + 100)   # Trace è€—æ—¶å˜é«˜
    service_anomaly[:, 12] = np.abs(service_anomaly[:, 12] * 40  + 150)   # Span Durationé•¿
    service_anomaly[:, 13] = np.abs(service_anomaly[:, 13] * 300 + 900)   # QPSä½ï¼ˆæœåŠ¡å™¨é™çº§ï¼‰
    service_anomaly[:, 14] = np.abs(service_anomaly[:, 14] * 2   + 5)     # 5xxé”™è¯¯ç‡é«˜

    # ç±»åˆ«4ï¼šç³»ç»Ÿçº§æ•…éšœå¼‚å¸¸ï¼ˆ20%ï¼‰
    n_pattern = n_anomaly - n_extreme - n_local - n_service
    pattern_anomaly = np.random.randn(n_pattern, n_features)
    pattern_anomaly = pattern_anomaly * 2 + np.array([
        150, 5, 3, 900, 55, 60, 98, 100,
        800, 500, 30, 100, 200, 1000, 10   # OpenTelemetryæœåŠ¡ç«¯æŒ‡æ ‡å¼‚å¸¸
    ])

    # åˆå¹¶æ•°æ®
    X = np.vstack([normal_data, extreme_anomaly, local_anomaly, service_anomaly, pattern_anomaly])
    y = np.hstack([np.zeros(n_normal), np.ones(n_anomaly)])

    # æ‰“ä¹±æ•°æ®
    indices = np.random.permutation(n_samples)
    X, y = X[indices], y[indices]

    # åˆ›å»ºç‰¹å¾åç§°ï¼ˆä¸­æ–‡å¯¹åº”ï¼‰
    feature_names = [
        # ä¼ ç»Ÿè¿è¥æŒ‡æ ‡ (Traditional Operational Metrics)
        'API_Response_Time_ms',      # APIå“åº”æ—¶é—´(æ¯«ç§’)
        'Processing_Delay_sec',      # æ‰¹å¤„ç†å»¶è¿Ÿ(ç§’)
        'Error_Rate_pct',            # é”™è¯¯ç‡(%)
        'Throughput_records_min',    # ååé‡(è®°å½•/åˆ†é’Ÿ)
        'CPU_Usage_pct',             # CPUä½¿ç”¨ç‡(%)
        'Memory_Usage_pct',          # å†…å­˜ä½¿ç”¨ç‡(%)
        'Data_Quality_Score',        # æ•°æ®è´¨é‡è¯„åˆ†
        'Concurrent_Connections',    # å¹¶å‘è¿æ¥æ•°

        # OpenTelemetry æœåŠ¡ä¾§æŒ‡æ ‡ (Server-side Metrics)
        'HTTP_P95_Latency_ms',       # HTTPè¯·æ±‚P95å»¶è¿Ÿ(æ¯«ç§’)
        'Peak_CPU_pct',              # CPUå³°å€¼(%)
        'Queue_Duration_ms',         # æ•°æ®é˜Ÿåˆ—æ—¶å»¶(æ¯«ç§’)
        'Cache_Hit_Ratio_pct',       # ç¼“å­˜å‘½ä¸­ç‡(%)
        'Span_Duration_ms',          # åˆ†å¸ƒå¼è¿½è¸ªSpanæ—¶é•¿(æ¯«ç§’)
        'Service_QPS',               # æœåŠ¡æŸ¥è¯¢ç‡ (QPS)
        'HTTP_5xx_Error_Rate_pct'    # HTTP 5xxé”™è¯¯ç‡(%)
    ]

    return X, y, feature_names


def generate_time_series_features(X, timestamps=None):
    """
    ç”Ÿæˆæ—¶åºç‰¹å¾ - é’ˆå¯¹å¤§è§„æ¨¡æ—¶åºé‡‘èæ•°æ®
    Time-series feature engineering for large-scale financial data
    """
    n_samples = X.shape[0]

    # å¦‚æœæ²¡æœ‰æä¾›æ—¶é—´æˆ³ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ—¶é—´æˆ³ (æ¯åˆ†é’Ÿä¸€ä¸ªæ•°æ®ç‚¹)
    if timestamps is None:
        start_time = datetime(2025, 1, 1)
        timestamps = [start_time + timedelta(minutes=i) for i in range(n_samples)]

    # æå–æ—¶é—´ç‰¹å¾
    time_features = pd.DataFrame({
        'hour': [t.hour for t in timestamps],
        'day_of_week': [t.weekday() for t in timestamps],
        'day_of_month': [t.day for t in timestamps],
        'is_weekend': [1 if t.weekday() >= 5 else 0 for t in timestamps],
        'is_business_hour': [1 if 9 <= t.hour < 18 else 0 for t in timestamps],
    })

    # å‘¨æœŸæ€§ç¼–ç  (sin/cos å˜æ¢é¿å…è¾¹ç•Œä¸è¿ç»­)
    time_features['hour_sin'] = np.sin(2 * np.pi * time_features['hour'] / 24)
    time_features['hour_cos'] = np.cos(2 * np.pi * time_features['hour'] / 24)
    time_features['day_sin'] = np.sin(2 * np.pi * time_features['day_of_week'] / 7)
    time_features['day_cos'] = np.cos(2 * np.pi * time_features['day_of_week'] / 7)

    # æ»‘åŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾ (æ•è·æ—¶åºä¾èµ–)
    window_sizes = [5, 10, 30]  # 5åˆ†é’Ÿã€10åˆ†é’Ÿã€30åˆ†é’Ÿçª—å£
    X_df = pd.DataFrame(X)

    for col in range(X.shape[1]):
        for window in window_sizes:
            # ç§»åŠ¨å¹³å‡
            time_features[f'feature_{col}_ma_{window}'] = X_df[col].rolling(window=window, min_periods=1).mean()
            # ç§»åŠ¨æ ‡å‡†å·®
            time_features[f'feature_{col}_std_{window}'] = X_df[col].rolling(window=window, min_periods=1).std().fillna(0)

    # åˆå¹¶åŸå§‹ç‰¹å¾å’Œæ—¶åºç‰¹å¾
    X_combined = np.hstack([X, time_features.values])

    return X_combined, time_features.columns.tolist()


def evaluate_time_series_performance(model, X_train, X_test, y_test, timestamps_test=None):
    """
    æ—¶åºæ•°æ®ä¸“ç”¨è¯„ä¼°æŒ‡æ ‡
    Time-series specific evaluation metrics
    """
    # åŸºç¡€æ€§èƒ½æŒ‡æ ‡
    y_pred = model.predict(X_test)
    y_scores = model.decision_function(X_test)

    metrics = {
        'auc': roc_auc_score(y_test, y_scores),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred)
    }

    # æ—¶åºä¸“ç”¨æŒ‡æ ‡: æ£€æµ‹å»¶è¿Ÿ (Detection Delay)
    # è®¡ç®—ä»å¼‚å¸¸å¼€å§‹åˆ°è¢«æ£€æµ‹å‡ºçš„å¹³å‡å»¶è¿Ÿ
    if timestamps_test is not None and len(timestamps_test) == len(y_test):
        anomaly_indices = np.where(y_test == 1)[0]
        detected_indices = np.where(y_pred == 1)[0]

        # ç®€åŒ–è®¡ç®—ï¼šç»Ÿè®¡æœ‰å¤šå°‘å¼‚å¸¸åœ¨åˆç†æ—¶é—´çª—å£å†…è¢«æ£€æµ‹
        metrics['detection_window_5min'] = 0
        for anomaly_idx in anomaly_indices:
            # æ£€æŸ¥åç»­5ä¸ªæ—¶é—´ç‚¹å†…æ˜¯å¦æ£€æµ‹åˆ°
            window = detected_indices[(detected_indices >= anomaly_idx) & (detected_indices < anomaly_idx + 5)]
            if len(window) > 0:
                metrics['detection_window_5min'] += 1

        if len(anomaly_indices) > 0:
            metrics['detection_window_5min'] /= len(anomaly_indices)

    # è¯¯æŠ¥ç‡ (False Positive Rate) - æ—¶åºæ•°æ®ä¸­çš„å…³é”®æŒ‡æ ‡
    tn = np.sum((y_test == 0) & (y_pred == 0))
    fp = np.sum((y_test == 0) & (y_pred == 1))
    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0

    return metrics


def compare_algorithms(X_train, X_test, y_test, contamination=0.05, enable_time_series_features=False):
    """
    æ¯”è¾ƒå¤šä¸ªå¼‚å¸¸æ£€æµ‹ç®—æ³• (é’ˆå¯¹å¤§è§„æ¨¡æ—¶åºé‡‘èæ•°æ®ä¼˜åŒ–)
    å‚æ•°:
    - enable_time_series_features: æ˜¯å¦å¯ç”¨æ—¶åºç‰¹å¾å·¥ç¨‹ (ä¼šå¢åŠ ç‰¹å¾ç»´åº¦)
    """
    print(f"æ•°æ®è§„æ¨¡: è®­ç»ƒé›† {X_train.shape}, æµ‹è¯•é›† {X_test.shape}")
    print(f"æ—¶åºç‰¹å¾å·¥ç¨‹: {'å¯ç”¨' if enable_time_series_features else 'ç¦ç”¨'}")
    # å®šä¹‰ç®—æ³• - é’ˆå¯¹å¤§è§„æ¨¡æ—¶åºé‡‘èæ•°æ®ä¼˜åŒ–
    # ä¼˜å…ˆè€ƒè™‘: 1) å¯æ‰©å±•æ€§ 2) è®­ç»ƒé€Ÿåº¦ 3) åœ¨çº¿å­¦ä¹ èƒ½åŠ›
    algorithms = {
        # â˜† æ¨èç”¨äºå¤§è§„æ¨¡æ—¶åºæ•°æ®çš„ç®—æ³•
        'Isolation Forest': IForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100,  # ä»200é™è‡³100ä»¥åŠ é€Ÿ
            max_samples='auto',  # è‡ªåŠ¨é‡‡æ ·ï¼Œé€‚åˆå¤§æ•°æ®
            n_jobs=-1 # å¹¶è¡Œè®­ç»ƒ
        ),
        'ECOD': ECOD(
            contamination=contamination,
            n_jobs=-1 # å¹¶è¡Œè®¡ç®—
        ),
        'HBOS': HBOS(
            contamination=contamination,
            n_bins=15,  # ä»20é™è‡³15ä»¥åŠ é€Ÿ
            tol=0.5  # å®¹å·®è®¾ç½®
        ),
        'COPOD': COPOD(
            contamination=contamination,
            n_jobs=-1
        ),
        # â—† ä¸­ç­‰æ¨è - æ€§èƒ½è‰¯å¥½ä½†å¯èƒ½åœ¨è¶…å¤§è§„æ¨¡æ•°æ®ä¸Šè¾ƒæ…¢
        'PCA': PCA(
            contamination=contamination,
            n_components=min(8, X_train.shape[1] // 2)  # åŠ¨æ€è®¾ç½®ä¸»æˆåˆ†æ•°
        ),
        # â–¼ ä¸æ¨èç”¨äºå¤§è§„æ¨¡æ•°æ® - ä»…ä½œå¯¹æ¯”
        'KNN': KNN(contamination=contamination, n_neighbors=10, n_jobs=-1),
        'LOF': LOF(contamination=contamination, n_neighbors=20, n_jobs=-1),
    }

    # å¯é€‰: æ·»åŠ æ·±åº¦å­¦ä¹ ç®—æ³• (é€‚åˆç¦»çº¿æ‰¹å¤„ç†)
    if AUTOENCODER_AVAILABLE:
        try:
            algorithms['AutoEncoder'] = AutoEncoder(
                contamination=contamination,
                hidden_neuron_list=[X_train.shape[1], max(10, X_train.shape[1]//2), max(10, X_train.shape[1]//2), X_train.shape[1]],
                epoch_num=30,  # ä»50é™è‡³30ä»¥åŠ é€Ÿ
                batch_size=64,  # ä»32å¢è‡³64ä»¥æå‡ååé‡
                verbose=0,
                random_state=42
            )
            print("  âœ… AutoEncoder å·²å¯ç”¨ (é€‚åˆç¦»çº¿åˆ†æ) ")
        except Exception as e:
            print(f"  âš ï¸ AutoEncoder åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    results = []
    print("\nå¼€å§‹ç®—æ³•æ¯”è¾ƒ (é’ˆå¯¹å¤§è§„æ¨¡æ—¶åºæ•°æ®ä¼˜åŒ–) ...")
    print("-" * 80)

    for name, model in algorithms.items():
        print(f"\nè®­ç»ƒ {name}...")
        start_time = time.time()
        try:
            # è®­ç»ƒ
            model.fit(X_train)
            # é¢„æµ‹
            y_pred = model.predict(X_test)
            y_scores = model.decision_function(X_test)
            # è®¡ç®—æŒ‡æ ‡
            training_time = time.time() - start_time
            # é¢„æµ‹æ—¶é—´
            start_pred = time.time()
            _ = model.predict(X_test[:100])
            prediction_time = (time.time() - start_pred) / 100 * 1000  # ms per sample

            auc = roc_auc_score(y_test, y_scores)
            precision = precision_score(y_test, y_pred)
            recall = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)

            # å¤§æ•°æ®ç‰¹å®šæŒ‡æ ‡: ååé‡ (æ¯ç§’å¤„ç†æ ·æœ¬æ•°)
            throughput = len(X_test) / (time.time() - start_pred) if (time.time() - start_pred) > 0 else 0

            # å†…å­˜æ•ˆç‡è¯„ä¼° (ç®€åŒ–ç‰ˆ)
            memory_friendly = "æ˜¯" if name in ['Isolation Forest', 'ECOD', 'HBOS', 'COPOD'] else "å¦"
            online_learning = "æ”¯æŒ" if name in ['HBOS'] else "ä¸æ”¯æŒ"
            results.append({
                'Algorithm': name,
                'AUC-ROC': auc,
                'Precision': precision,
                'Recall': recall,
                'F1-Score': f1,
                'Training Time (s)': training_time,
                'Prediction Time (ms)': prediction_time,
                'Throughput (samples/s)': throughput,
                'Memory Efficient': memory_friendly,
                'Scalability': 'ä¼˜ç§€' if name in ['Isolation Forest', 'ECOD'] else 'ä¸€èˆ¬' if name in ['HBOS', 'COPOD', 'PCA'] else 'å·®'
            })
            print(f"  AUC: {auc:.4f}, F1: {f1:.4f}, è®­ç»ƒ: {training_time:.2f}s, ååé‡: {throughput:.0f} samples/s")

        except Exception as e:
            print(f"  âŒ å¤±è´¥: {str(e)}")
            continue

    return pd.DataFrame(results)


def plot_comparison_results(results_df, output_path='algorithm_comparison.png'):
    """
    å¯è§†åŒ–æ¯”è¾ƒç»“æœ
    """
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # 1. æ€§èƒ½æŒ‡æ ‡æ¯”è¾ƒ
    metrics = ['AUC-ROC', 'Precision', 'Recall', 'F1-Score']
    results_plot = results_df.set_index('Algorithm')[metrics]

    ax1 = axes[0, 0]
    results_plot.plot(kind='bar', ax=ax1, width=0.8)
    ax1.set_title('Performance Metrics Comparison', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Score', fontsize=12)
    ax1.legend(loc='lower right')
    ax1.grid(axis='y', alpha=0.3)
    ax1.set_ylim([0, 1.1])

    # 2. AUC-ROC æ’å
    ax2 = axes[0, 1]
    sorted_auc = results_df.sort_values('AUC-ROC', ascending=True)
    colors = plt.cm.RdYlGn(sorted_auc['AUC-ROC'].values)
    ax2.barh(sorted_auc['Algorithm'], sorted_auc['AUC-ROC'], color=colors)
    ax2.set_xlabel('AUC-ROC Score', fontsize=12)
    ax2.set_title('AUC-ROC Ranking', fontsize=14, fontweight='bold')
    ax2.grid(axis='x', alpha=0.3)

    # 3. è®­ç»ƒæ—¶é—´æ¯”è¾ƒ
    ax3 = axes[1, 0]
    sorted_time = results_df.sort_values('Training Time (s)')
    ax3.bar(range(len(sorted_time)), sorted_time['Training Time (s)'],
            color='steelblue', alpha=0.7)
    ax3.set_xticks(range(len(sorted_time)))
    ax3.set_xticklabels(sorted_time['Algorithm'], rotation=45, ha='right')
    ax3.set_ylabel('Training Time (seconds)', fontsize=12)
    ax3.set_title('Training Time Comparison', fontsize=14, fontweight='bold')
    ax3.grid(axis='y', alpha=0.3)

    # 4. Precision vs Recall
    ax4 = axes[1, 1]
    for idx, row in results_df.iterrows():
        ax4.scatter(row['Recall'], row['Precision'], s=200, alpha=0.6)
        ax4.annotate(row['Algorithm'],
                     (row['Recall'], row['Precision']),
                     fontsize=9, ha='center')
    ax4.set_xlabel('Recall', fontsize=12)
    ax4.set_ylabel('Precision', fontsize=12)
    ax4.set_title('Precision vs Recall Trade-off', fontsize=14, fontweight='bold')
    ax4.grid(alpha=0.3)
    ax4.set_xlim([0, 1.1])
    ax4.set_ylim([0, 1.1])

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š å›¾è¡¨å·²ä¿å­˜: {output_path}")

    return fig

def generate_markdown_report(results_df, X_test, y_test, feature_names, output_path='anomaly_detection_report.md'):
    """
    ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š
    """

    report = f"""# ğŸ¦ é‡‘èè¿è¥æ•°æ®å¼‚å¸¸æ£€æµ‹ç®—æ³•æ¯”è¾ƒæŠ¥å‘Š
## Financial Operational Data Anomaly Detection Algorithm Comparison

**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## 1. æ‰§è¡Œæ‘˜è¦ (Executive Summary)

æœ¬æŠ¥å‘Šé’ˆå¯¹é‡‘èè¿è¥æ•°æ® (éäº¤æ˜“æ•°æ®) çš„å¤šç§å¼‚å¸¸æ£€æµ‹åœºæ™¯ï¼Œå¯¹æ¯”äº† {len(results_df)} ç§ä¸»æµå¼‚å¸¸æ£€æµ‹ç®—æ³•çš„æ€§èƒ½è¡¨ç°ã€‚

### æ•°æ®é›†ç‰¹å¾
- **æ ·æœ¬æ•°é‡:** {len(X_test)} (æµ‹è¯•é›†)
- **ç‰¹å¾ç»´åº¦:** {len(feature_names)}
- **å¼‚å¸¸æ¯”ä¾‹:** {y_test.sum() / len(y_test) * 100:.2f}%
- **æ•°æ®ç±»å‹:** é‡‘èè¿è¥ç›‘æ§æ•°æ® (ç³»ç»Ÿæ€§èƒ½ã€æ•°æ®è´¨é‡ã€ä¸šåŠ¡æŒ‡æ ‡ç­‰)

### ç‰¹å¾åˆ—è¡¨
"""

    for i, fname in enumerate(feature_names, 1):
        report += f"{i}. `{fname}`\n"

    report += "\n---\n\n## 2. ç®—æ³•æ€§èƒ½å¯¹æ¯” (Algorithm Performance Comparison)\n\n"

    # æ’åºå¹¶æ ‡æ³¨
    results_sorted = results_df.sort_values('AUC-ROC', ascending=False)

    report += "### 2.1 ç»¼åˆæ’å (Overall Ranking)\n\n"
    report += "| æ’å | ç®—æ³• | AUC-ROC | F1-Score | Precision | Recall |\n"
    report += "|---|---|---|---|---|---|\n"

    for idx, (_, row) in enumerate(results_sorted.iterrows(), 1):
        medal = "ğŸ¥‡" if idx == 1 else "ğŸ¥ˆ" if idx == 2 else "ğŸ¥‰" if idx == 3 else f"{idx}"
        report += f"| {medal} | **{row['Algorithm']}** | {row['AUC-ROC']:.4f} | {row['F1-Score']:.4f} | {row['Precision']:.4f} | {row['Recall']:.4f} |\n"

    report += "\n### 2.2 è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ (Detailed Metrics)\n\n"
    report += results_df.to_markdown(index=False, floatfmt=".4f")

    # æœ€ä½³ç®—æ³•
    best_algo = results_sorted.iloc[0]
    report += f"\n\n### 2.3 æ¨èç®—æ³• (Recommended Algorithm)\n\n"
    report += f"**ğŸ† æœ€ä½³ç»¼åˆæ€§èƒ½: {best_algo['Algorithm']}**\n"
    report += f"- **AUC-ROC:** {best_algo['AUC-ROC']:.4f}\n"
    report += f"- **F1-Score:** {best_algo['F1-Score']:.4f}\n"
    report += f"- **Precision:** {best_algo['Precision']:.4f}\n"
    report += f"- **Recall:** {best_algo['Recall']:.4f}\n"
    report += f"- **è®­ç»ƒæ—¶é—´:** {best_algo['Training Time (s)']:.2f} ç§’\n"
    report += f"- **é¢„æµ‹æ—¶é—´:** {best_algo['Prediction Time (ms)']:.4f} æ¯«ç§’/æ ·æœ¬\n"

    report += "\n---\n\n## 3. ç®—æ³•ç‰¹ç‚¹åˆ†æ (Algorithm Characteristics)\n\n"

    algo_analysis = {
        'Isolation Forest': {
            'é€‚ç”¨åœºæ™¯': 'é«˜ç»´æ•°æ®ã€å¤§è§„æ¨¡æ•°æ®é›†ã€å®æ—¶æ£€æµ‹',
            'ä¼˜åŠ¿': 'è®­ç»ƒé€Ÿåº¦å¿«ã€å†…å­˜æ•ˆç‡é«˜ã€å¯¹å¼‚å¸¸ç±»å‹ä¸æ•æ„Ÿã€å¯è§£é‡Šæ€§å¥½',
            'åŠ£åŠ¿': 'å¯¹å±€éƒ¨å¼‚å¸¸æ£€æµ‹ä¸å¦‚åŸºäºå¯†åº¦çš„æ–¹æ³•',
            'æ¨èæŒ‡æ•°': 'â­â­â­â­â­',
            'é‡‘èåœºæ™¯': 'ç³»ç»Ÿæ—¥å¿—å¼‚å¸¸ã€API æ€§èƒ½ç›‘æ§ã€æ‰¹å¤„ç†ä½œä¸šç›‘æ§'
        },
        'ECOD': {
            'é€‚ç”¨åœºæ™¯': 'é«˜ç»´æ•°æ®ã€æ— ç›‘ç£åœºæ™¯ã€å¿«é€Ÿéƒ¨ç½²',
            'ä¼˜åŠ¿': 'é€Ÿåº¦æå¿«ã€æ— éœ€è°ƒå‚ã€å¯¹é«˜ç»´æ•°æ®å‹å¥½',
            'åŠ£åŠ¿': 'å¯¹å¤æ‚çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›æœ‰é™',
            'æ¨èæŒ‡æ•°': 'â­â­â­â­â­',
            'é‡‘èåœºæ™¯': 'å®æ—¶ç›‘æ§å‘Šè­¦ã€æ•°æ®è´¨é‡æ£€æŸ¥ã€è¿ç»´æŒ‡æ ‡ç›‘æ§'
        },
        'COPOD': {
            'é€‚ç”¨åœºæ™¯': 'å¤šå˜é‡å…³è”å¼‚å¸¸ã€ç¦»çº¿æ•°æ®',
            'ä¼˜åŠ¿': 'æ— éœ€è®­ç»ƒã€é€Ÿåº¦å¿«ã€å¯è§£é‡Šæ€§å¼º',
            'åŠ£åŠ¿': 'å¯¹å¤æ‚éçº¿æ€§å…³ç³»æ£€æµ‹æœ‰é™',
            'æ¨èæŒ‡æ•°': 'â­â­â­â­',
            'é‡‘èåœºæ™¯': 'å¤šç³»ç»Ÿå…³è”å¼‚å¸¸ã€è·¨æŒ‡æ ‡å¼‚å¸¸æ£€æµ‹'
        },
        'LOF': {
            'é€‚ç”¨åœºæ™¯': 'å±€éƒ¨å¼‚å¸¸ã€å¯†åº¦å˜åŒ–åŒºåŸŸ',
            'ä¼˜åŠ¿': 'å¯¹å±€éƒ¨å¼‚å¸¸æ•æ„Ÿã€é€‚åˆéå‡åŒ€åˆ†å¸ƒ',
            'åŠ£åŠ¿': 'è®¡ç®—å¤æ‚åº¦é«˜ã€å¯¹å‚æ•°æ•æ„Ÿ',
            'æ¨èæŒ‡æ•°': 'â­â­â­',
            'é‡‘èåœºæ™¯': 'ç”¨æˆ·è¡Œä¸ºå¼‚å¸¸ã€äº¤æ˜“æ¨¡å¼å¼‚å¸¸'
        },
        'KNN': {
            'é€‚ç”¨åœºæ™¯': 'ä¸­å°è§„æ¨¡æ•°æ®ã€å±€éƒ¨å¼‚å¸¸',
            'ä¼˜åŠ¿': 'ç®€å•ç›´è§‚ã€æ— éœ€è®­ç»ƒ',
            'åŠ£åŠ¿': 'å¯¹å¤§è§„æ¨¡æ•°æ®æ€§èƒ½å·®ã€å¯¹å‚æ•°æ•æ„Ÿ',
            'æ¨èæŒ‡æ•°': 'â­â­â­',
            'é‡‘èåœºæ™¯': 'å°æ‰¹é‡æ•°æ®è´¨é‡æ£€æŸ¥ã€ç¦»çº¿åˆ†æ'
        },
        'OCSVM': {
            'é€‚ç”¨åœºæ™¯': 'æ¸…æ™°è¾¹ç•Œçš„å¼‚å¸¸ã€å°æ ·æœ¬åœºæ™¯',
            'ä¼˜åŠ¿': 'ç†è®ºåŸºç¡€æ‰å®ã€å¯¹å™ªå£°é²æ£’',
            'åŠ£åŠ¿': 'è®­ç»ƒæ…¢ã€å‚æ•°è°ƒä¼˜å›°éš¾ã€ä¸é€‚åˆå¤§è§„æ¨¡æ•°æ®',
            'æ¨èæŒ‡æ•°': 'â­â­â­',
            'é‡‘èåœºæ™¯': 'å…³é”®ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§ã€åˆè§„æ£€æŸ¥'
        },
        'PCA': {
            'é€‚ç”¨åœºæ™¯': 'é«˜ç»´æ•°æ®é™ç»´ã€çº¿æ€§ç›¸å…³å¼‚å¸¸',
            'ä¼˜åŠ¿': 'é™ç»´æ•ˆæœå¥½ã€å¯è§£é‡Šæ€§å¼º',
            'åŠ£åŠ¿': 'åªèƒ½æ£€æµ‹çº¿æ€§æ¨¡å¼å¼‚å¸¸',
            'æ¨èæŒ‡æ•°': 'â­â­â­â­',
            'é‡‘èåœºæ™¯': 'å¤šæŒ‡æ ‡ç»¼åˆç›‘æ§ã€ç‰¹å¾å‹ç¼©'
        },
        'MCD': {
            'é€‚ç”¨åœºæ™¯': 'å¤šå˜é‡é«˜æ–¯åˆ†å¸ƒæ•°æ®',
            'ä¼˜åŠ¿': 'å¯¹ç¦»ç¾¤ç‚¹é²æ£’ã€ç»Ÿè®¡ç†è®ºæ”¯æ’‘',
            'åŠ£åŠ¿': 'å‡è®¾æ•°æ®ç¬¦åˆé«˜æ–¯åˆ†å¸ƒã€è®¡ç®—å¤æ‚',
            'æ¨èæŒ‡æ•°': 'â­â­â­',
            'é‡‘èåœºæ™¯': 'é£é™©æŒ‡æ ‡ç›‘æ§ã€ç¨³æ€ç³»ç»Ÿç›‘æ§'
        },
        'HBOS': {
            'é€‚ç”¨åœºæ™¯': 'å¿«é€Ÿç­›é€‰ã€å¤§è§„æ¨¡æ•°æ®',
            'ä¼˜åŠ¿': 'é€Ÿåº¦æå¿«ã€å¯è§£é‡Šæ€§å¼º',
            'åŠ£åŠ¿': 'å‡è®¾ç‰¹å¾ç‹¬ç«‹ã€æ£€æµ‹èƒ½åŠ›æœ‰é™',
            'æ¨èæŒ‡æ•°': 'â­â­â­â­',
            'é‡‘èåœºæ™¯': 'åˆæ­¥ç­›é€‰ã€å®æ—¶é¢„è­¦'
        }
    }

    for algo, analysis in algo_analysis.items():
        if algo in results_df['Algorithm'].values:
            report += f"\n### {algo}\n\n"
            for key, value in analysis.items():
                report += f"- **{key}:** {value}\n"

    report += "\n---\n\n## 4. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å»ºè®® (Production Deployment Recommendations)\n\n"
    report += """### 4.1 åœºæ™¯é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èç®—æ³• | ç†ç”± |
|---|---|---|
| **å®æ—¶ç›‘æ§ç³»ç»Ÿ** | Isolation Forest, ECOD | é¢„æµ‹é€Ÿåº¦å¿«ï¼Œå†…å­˜æ•ˆç‡é«˜ |
| **æ‰¹é‡ç¦»çº¿åˆ†æ** | LOF, COPOD | æ£€æµ‹ç²¾åº¦é«˜ï¼Œé€‚åˆæ·±åº¦åˆ†æ |
| **é«˜ç»´æ•°æ®** | ECOD, Isolation Forest, PCA | å¯¹é«˜ç»´ç‰¹å¾é²æ£’æ€§å¥½ |
| **å±€éƒ¨å¼‚å¸¸æ£€æµ‹** | LOF, KNN | å¯¹å¯†åº¦å˜åŒ–æ•æ„Ÿ |
| **å¯è§£é‡Šæ€§è¦æ±‚é«˜** | COPOD, HBOS, Isolation Forest | å¯æä¾›ç‰¹å¾è´¡çŒ®åº¦ |
| **å¤§è§„æ¨¡æ•°æ®** | Isolation Forest, ECOD, HBOS | å¯æ‰©å±•æ€§å¥½ |

### 4.2 æ··åˆç­–ç•¥å»ºè®®

é’ˆå¯¹é‡‘èè¿è¥æ•°æ®çš„å¤æ‚æ€§ï¼Œå»ºè®®é‡‡ç”¨**å¤šç®—æ³•é›†æˆç­–ç•¥**ï¼š

1. **ç¬¬ä¸€å±‚ï¼šå¿«é€Ÿç­›é€‰**
   - ä½¿ç”¨ ECOD æˆ– HBOS è¿›è¡Œå®æ—¶å¿«é€Ÿç­›é€‰
   - é˜ˆå€¼è®¾ç½®åå®½æ¾ï¼Œç¡®ä¿é«˜å¬å›ç‡

2. **ç¬¬äºŒå±‚ï¼šç²¾ç¡®æ£€æµ‹**
   - å¯¹ç¬¬ä¸€å±‚ç­›é€‰å‡ºçš„å¯ç–‘æ ·æœ¬ï¼Œä½¿ç”¨ Isolation Forest æˆ– LOF è¿›è¡Œç²¾ç¡®æ£€æµ‹
   - å¹³è¡¡ Precision å’Œ Recall

3. **ç¬¬ä¸‰å±‚ï¼šä¸“å®¶éªŒè¯**
   - é«˜é£é™©å¼‚å¸¸äººå·¥å¤æ ¸
   - æŒç»­æ ‡æ³¨ä»¥æ”¯æŒæœªæ¥ç›‘ç£/åŠç›‘ç£å­¦ä¹ 

### 4.3 ç›‘æ§ä¸è°ƒä¼˜

- **å®šæœŸè¯„ä¼°:** æ¯æœˆé‡æ–°è¯„ä¼°ç®—æ³•æ€§èƒ½ï¼Œæ ¹æ®æ•°æ®åˆ†å¸ƒå˜åŒ–è°ƒæ•´
- **A/B æµ‹è¯•:** åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åŒæ—¶è¿è¡Œå¤šä¸ªç®—æ³•ï¼Œå¯¹æ¯”æ•ˆæœ
- **é˜ˆå€¼åŠ¨æ€è°ƒæ•´:** æ ¹æ®ä¸šåŠ¡å®¹å¿åº¦å’Œå‘Šè­¦ç–²åŠ³åº¦åŠ¨æ€è°ƒæ•´é˜ˆå€¼
- **ç‰¹å¾å·¥ç¨‹:** æŒç»­ä¼˜åŒ–ç‰¹å¾é€‰æ‹©å’Œå·¥ç¨‹ï¼Œæå‡æ£€æµ‹æ•ˆæœ

---

## 5. æŠ€æœ¯å®ç°è¦ç‚¹ (Technical Implementation Notes)

### 5.1 æ•°æ®é¢„å¤„ç†
```python
# æ ‡å‡†åŒ–å¤„ç†

### 5.2 å‚æ•°è°ƒä¼˜å»ºè®®
- **contamination**: å»ºè®®0.05-0.10
- **n_neighbors (KNN/LOF)*: å»ºè®®è®¾ç½®ä¸ºæ ·æœ¬æ•°1-5%
- **n_estimators (Isolation Forest)**: å»ºè®®100ï½300

"""

    with open(out_path, 'w', encoding = 'utf-8') as f:
        f.write(report)

    print(f"\n Markdown æŠ¥å‘Šå·²ç”Ÿæˆ {output_path}")
    return report

def main():
    X, y, feature_names = generate_financial_operational_data( 
        n_samples= 10000,
        n_features=15,
        contamination=0.05
    )

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state= 42, stratify=y
    )

    results_df = compare_algorithms(X_train, X_test, y_test, contamination= 0.05)

    print(results_df.to_string(index=False))

    plot_comparison_results(results_df, output_path = './comparison.png')

    generate_markdown_report(
        results_df,
        X_test,
        y_test,
        feature_names,
        output_path='./report.md'
    )

    return results_df, X_test, y_test

if __name__ == "__main__":
    results_df, X_test, y_test = main()